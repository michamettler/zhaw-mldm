{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/michamettler/zhaw-mldm/blob/main/Copy_of_L01_Data_Cleaning_ASSIGNMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8fDg0Zyh3D8"
   },
   "source": [
    "# Lab 01 Data Cleaning and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y20uIgmUh9D_"
   },
   "source": [
    "In this Lab, we will work with official COVID19 case data published by the Swiss government. The data can be found [here](https://github.com/openZH/covid_19/tree/master#swiss-cantons-and-principality-of-liechtenstein-unified-dataset).\n",
    "\n",
    "It has been partially adapted from [this](https://datagy.io/pandas-data-cleaning/) tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70M0L7qqdFpX"
   },
   "source": [
    "**Note:** We will mark your tasks with 🚨 emoji. In this lab the first such task will appear relatively late."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "su744hkOjT_q"
   },
   "source": [
    "## Fetching the Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmHEwCJWiS1p"
   },
   "source": [
    "First, we will download the raw data. Don't worry; you do not need to understand this command. The data will be saved as `swiss_covid_data.csv` in your current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YGkGkFqrg09V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-11-29 16:36:36--  https://raw.githubusercontent.com/alisarupenyan/MLDMlabsHS2024/refs/heads/main/lab01_dirty_data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3235140 (3.1M) [text/plain]\n",
      "Saving to: 'swiss_covid_data.csv'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1% 2.69M 1s\n",
      "    50K .......... .......... .......... .......... ..........  3% 2.89M 1s\n",
      "   100K .......... .......... .......... .......... ..........  4% 9.10M 1s\n",
      "   150K .......... .......... .......... .......... ..........  6% 3.86M 1s\n",
      "   200K .......... .......... .......... .......... ..........  7% 18.7M 1s\n",
      "   250K .......... .......... .......... .......... ..........  9% 4.40M 1s\n",
      "   300K .......... .......... .......... .......... .......... 11% 55.4M 1s\n",
      "   350K .......... .......... .......... .......... .......... 12% 7.43M 1s\n",
      "   400K .......... .......... .......... .......... .......... 14% 81.0M 0s\n",
      "   450K .......... .......... .......... .......... .......... 15% 53.4M 0s\n",
      "   500K .......... .......... .......... .......... .......... 17% 7.39M 0s\n",
      "   550K .......... .......... .......... .......... .......... 18% 11.3M 0s\n",
      "   600K .......... .......... .......... .......... .......... 20% 15.2M 0s\n",
      "   650K .......... .......... .......... .......... .......... 22% 60.2M 0s\n",
      "   700K .......... .......... .......... .......... .......... 23% 62.8M 0s\n",
      "   750K .......... .......... .......... .......... .......... 25% 33.3M 0s\n",
      "   800K .......... .......... .......... .......... .......... 26% 21.6M 0s\n",
      "   850K .......... .......... .......... .......... .......... 28%  229M 0s\n",
      "   900K .......... .......... .......... .......... .......... 30%  161M 0s\n",
      "   950K .......... .......... .......... .......... .......... 31% 41.7M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 33% 23.4M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 34% 11.3M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 36% 68.1M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 37% 17.8M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 39% 33.1M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 41%  108M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 42% 34.9M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 44% 19.9M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 45%  190M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 47% 20.7M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 49% 29.5M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 50% 31.8M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 52%  120M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 53% 49.1M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 55% 11.5M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 56%  165M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 58% 84.8M 0s\n",
      "  1850K .......... .......... .......... .......... .......... 60%  152M 0s\n",
      "  1900K .......... .......... .......... .......... .......... 61% 84.0M 0s\n",
      "  1950K .......... .......... .......... .......... .......... 63% 43.1M 0s\n",
      "  2000K .......... .......... .......... .......... .......... 64% 63.8M 0s\n",
      "  2050K .......... .......... .......... .......... .......... 66% 91.2M 0s\n",
      "  2100K .......... .......... .......... .......... .......... 68% 41.7M 0s\n",
      "  2150K .......... .......... .......... .......... .......... 69% 70.2M 0s\n",
      "  2200K .......... .......... .......... .......... .......... 71%  107M 0s\n",
      "  2250K .......... .......... .......... .......... .......... 72% 33.2M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 74% 63.2M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 75% 93.9M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 77% 56.2M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 79%  103M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 80% 37.5M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 82% 77.0M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 83% 3.00M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 85% 65.4M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 87%  122M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 88% 75.9M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 90% 24.1M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 91% 8.14M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 93% 96.3M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 94% 48.4M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 96% 90.2M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 98% 81.8M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 99% 76.1M 0s\n",
      "  3150K .........                                             100% 82.3M=0.2s\n",
      "\n",
      "2024-11-29 16:36:37 (18.6 MB/s) - 'swiss_covid_data.csv' saved [3235140/3235140]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/alisarupenyan/MLDMlabsHS2024/refs/heads/main/lab01_dirty_data.csv -O swiss_covid_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laWhb0nSinz-"
   },
   "source": [
    "Next, we try to inspect the downloaded data to get an understanding of its format. The command below will display the first 6 lines of the downloaded file.\n",
    "\n",
    "Even though `.csv` stands for \"comma separated values\", often times people distribute `.csv` files with different separator characters. In this case, everything seems to be in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fl5ISREjhhCG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"head\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n"
     ]
    }
   ],
   "source": [
    "!head -n 6 swiss_covid_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_qtzlLwjbHB"
   },
   "source": [
    "In this lab, we will use the `pandas` library to analyze and clean this dataset. This library is particularly well suited to work with tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5rhts6vUjvr-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # we import the library with the short-name `pd`, a convention you will find everywhere online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF8FUL2ykCVw"
   },
   "source": [
    "Next, we will read the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "outDL0D6j8a-"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'swiss_covid_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m covid_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mswiss_covid_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mettl\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mettl\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mettl\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\mettl\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\mettl\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\mettl\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\mettl\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'swiss_covid_data.csv'"
     ]
    }
   ],
   "source": [
    "covid_data = pd.read_csv('swiss_covid_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fa2BO75kiSD"
   },
   "source": [
    "Note that if you ever work with a `.csv` that does not include the column names in the first line, or has a different separating character, you can specify these in `pd.read_csv` with the `sep` and `header` keyword arguments. For more options, you can refer to the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woOoUG7ClWCp"
   },
   "source": [
    "The data is loaded as a `pd.Dataframe` which exposes many useful methods for this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNTIWnXclj2K"
   },
   "source": [
    "## Inspecting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxN_DAo0loVg"
   },
   "source": [
    "We can use the `.head()` method to get a quick idea of the kind of data we are dealing with. It will return the first `n=5` rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fjO63aUolo8v"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'covid_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m covid_data\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'covid_data' is not defined"
     ]
    }
   ],
   "source": [
    "covid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-oFMdinsYxw"
   },
   "source": [
    "Next, it is important to get a good sense what the different columns represent. Some of them have intuitive names, such as `date` and `time`, but names like `ncumul_conf` can be inscrutable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izSwTMH0tAaZ"
   },
   "source": [
    "For convenience, we copied the [data description](https://github.com/openZH/covid_19/blob/master/README.md#swiss-cantons-and-principality-of-liechtenstein-unified-dataset) below.\n",
    "Be sure to study it carefully. Understanding the meaning of the data you are working with is crucial when deciding how to deal with missing values and other aspect of data cleanup!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spjSOwUWsng4"
   },
   "source": [
    "\n",
    "**Metadata**\n",
    "\n",
    "| Field Name          | Description                                | Format     | Note |\n",
    "|---------------------|--------------------------------------------|------------|------|\n",
    "| __date__              | Date of notification                       | YYYY-MM-DD | |\n",
    "| __time__                 | Time of notification                       | HH:MM      | |\n",
    "| __abbreviation_canton_and_fl__  | Abbreviation of the reporting canton       | Text       | |\n",
    "| __ncumul_tested__      | Reported number of tests performed as of date| Number     | Irrespective of canton of residence |\n",
    "| __ncumul_conf__          | Reported number of confirmed cases as of date| Number     | Only cases that reside in the current canton |\n",
    "| __new_hosp__        | new hospitalisations since last date | Number     | Irrespective of canton of residence |\n",
    "| __current_hosp__       | Reported number of hospitalised patients on date | Number     | Irrespective of canton of residence |\n",
    "| __current_icu__       | Reported number of hospitalised patients in ICUs on date| Number     | Irrespective of canton of residence |\n",
    "| __current_vent__        | Reported number of patients requiring invasive ventilation on date | Number     | Irrespective of canton of residence |\n",
    "| __ncumul_released__     |Reported number of patients released from hospitals or reported recovered as of date| Number     | Irrespective of canton of residence |\n",
    "| __ncumul_deceased__     |Reported number of deceased as of date| Number     | Only cases that reside in the current canton |\n",
    "| __source__              | Source of the information                  | href       | |\n",
    "| __current_isolated__       | Reported number of isolated persons on date          | Number       | Infected persons, who are not hospitalised |\n",
    "| __current_quarantined__    | Reported number of quarantined persons on date       | Number       | Persons, who were in 'close contact' with an infected person, while that person was infectious, and are not hospitalised themselves |\n",
    "| __current_quarantined_riskareatravel__    | Reported number of quarantined persons on date       | Number       | People arriving in Switzerland from [certain countries and areas](https://www.bag.admin.ch/bag/en/home/krankheiten/ausbrueche-epidemien-pandemien/aktuelle-ausbrueche-epidemien/novel-cov/empfehlungen-fuer-reisende/quarantaene-einreisende.html), who are required to go into quarantine.  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUNui20Vs2_7"
   },
   "source": [
    "Another useful method to get a quick feel for the data at hand is `.describe()` which produces a table of statistics about the different columns. By default it will only include numeric columns, but you can set `include='all'` to get an overview of all columns. Note that when computing statistics such as the mean, `NaN` values will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYWGO9TflzcD"
   },
   "outputs": [],
   "source": [
    "covid_data.describe()\n",
    "# try setting `include='all'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o67juEu03jwS"
   },
   "source": [
    "## Dealing with Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXARtRbm6ZUr"
   },
   "source": [
    "When reading the original `swiss_covid_data.csv` every cell that did not have a value was assigned the value `NaN` (not a number).\n",
    "\n",
    "When we apply the `.isna()` method to our data, every cell of the result will contain a boolean value indicating whether that cell had a `Nan` (or `None` or similar false-y value) in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R47RwO2N3muM"
   },
   "outputs": [],
   "source": [
    "covid_data.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54PQWJn07gV7"
   },
   "source": [
    "We can count the number of missing values in each column by summing over all rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYGocGKa7rpe"
   },
   "outputs": [],
   "source": [
    "covid_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lp1UXoq8Okf"
   },
   "source": [
    "*Note*: You can specify `.sum(axis=1)` to sum over columns instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdwsqvMR8hd6"
   },
   "source": [
    "We notice that the `current_quarantined_total` has 24040 missing values, which corresponds to the total number of entries in the dataframe.\n",
    "\n",
    "We can re-verify this by inspecting the unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6PROJI88_09"
   },
   "outputs": [],
   "source": [
    "covid_data['current_quarantined_total'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4qOB_7q9GI6"
   },
   "source": [
    "We therefore suggest to drop this column from the data entirely.\n",
    "\n",
    "This can be achieved by the `.drop()` method, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0WuyV5Q9Q2G"
   },
   "outputs": [],
   "source": [
    "covid_data = covid_data.drop(columns=['current_quarantined_total'])\n",
    "covid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iE4NlCHASww"
   },
   "source": [
    "Generally, there are two approaches to deal with missing values. We can either ignore them, for example by throwing out rows that contain them, or we can try to impute a \"reasonable\" value.\n",
    "\n",
    "Of course, what is \"reasonable\" heavily depends on the data at hand!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFFtNXsgAodu"
   },
   "source": [
    "Let us first consider the easier approach: dropping rows that have missing entries. This can be achieved using the `.dropna()` method.\n",
    "\n",
    "By default it will remove all rows that have at least one missing value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYRpPYtr9lap"
   },
   "outputs": [],
   "source": [
    "covid_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejYXNdoBBe9A"
   },
   "source": [
    "We can see that this is too aggressive as it leaves us with only 68 out our original 24032 rows!\n",
    "\n",
    "We can be more lax about dropping rows by either setting `how='all'`, which will only drop rows that consist entirely of missing values, or `thresh=3` (or any other integer value) which will drop rows with fewer than 3 non-missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZClmLq5CFoX"
   },
   "outputs": [],
   "source": [
    "covid_data.dropna(thresh=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBddUuNBxPP-"
   },
   "source": [
    "### Replacing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoIwwfzgDoRy"
   },
   "source": [
    "We will now focus on imputing missing values.\n",
    "\n",
    "Our data contains broadly speaking 3 types of values:\n",
    "* direct and cumulative counts of populations (numerical)\n",
    "* `date` and `time`\n",
    "* categorical `abbreviation_canton_and_fl` and `source`\n",
    "\n",
    "We already know that `abbreviation_canton_and_fl` and `date` do not have any missing values.\n",
    "\n",
    "`source` has only 2 missing values, which we will ignore for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiR4qeRjFAnG"
   },
   "source": [
    "We will first focus on the `time` column. Looking at the dataframe console printouts, we can already guess that many rows have the value `\"00:00\"` as their time.\n",
    "\n",
    "We can confirm this by counting how many times each unique `time` value appears:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZoxuQnaFcBb"
   },
   "outputs": [],
   "source": [
    "covid_data['time'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njQA3zUOGRJU"
   },
   "source": [
    "We will therefore decide to fill the missing `time` values with\n",
    "`\"00:00\"`. This can be achieved by the `.fillna()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wk0tyeVzGQJ4"
   },
   "outputs": [],
   "source": [
    "covid_data['time'] = covid_data['time'].fillna(\"00:00\")\n",
    "covid_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4KXhmoMG8lb"
   },
   "source": [
    "Note that we specifically update the `time` column with a version where the missing values have been replaced. If we instead wrote `covid_data.fillna(\"00:00\")` then every missing value in the entire dataframe would be replaced by `\"00:00\"`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "945Jgvt_Dwz8"
   },
   "source": [
    "**(Side Note)**: If we inspect the data more closely, we can see that each canton seems to have a specific time that they usually submit the data, so filling in \"00:00\" agnostically might not be the perfect solution but we will consider it good enough for this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxYq0v93EI7S"
   },
   "source": [
    "Next, we will fill in current counts, such as `current_hosp`. In the lecture you have seen different approaches to determine a good value to impute. Intuitively, if we do not know how many people are currently in the hospital, a simple estimate would be to use the number of people hospitalised the day before, or the last known value in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjIIInBqIHBP"
   },
   "source": [
    "This can be achieved in pandas by using the `.ffill()` (forward-fill) method. The code sniplet below shows a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyy4zhPmIjpx"
   },
   "outputs": [],
   "source": [
    "ffill_example = pd.DataFrame({\"values\": [1, 2, 3, None, None, 6]})\n",
    "ffill_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccRvHuYoI06P"
   },
   "outputs": [],
   "source": [
    "ffill_example.ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7PFDwu2JHGy"
   },
   "source": [
    "For our covid data this will become a little more intricate. First, we have to make sure that the data is sorted by date and time to make sure we actually forward fill the last known values. Second, we will have to do this for each canton individually, as it makes no sense to substitute a known value from Geneva in Zurich. We show how to achieve this in the next cell and unpack it afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aBMcfTVFzep"
   },
   "outputs": [],
   "source": [
    "ffill_columns = [\n",
    "  'ncumul_tested',\n",
    "  'ncumul_conf',\n",
    "  'current_hosp',\n",
    "  'current_icu',\n",
    "  'current_vent',\n",
    "  'ncumul_released',\n",
    "  'ncumul_deceased',\n",
    "  'current_isolated',\n",
    "  'current_quarantined',\n",
    "  'current_quarantined_riskareatravel',\n",
    "]\n",
    "for canton in covid_data['abbreviation_canton_and_fl'].unique():\n",
    "  covid_data.loc[covid_data['abbreviation_canton_and_fl'] == canton, ffill_columns] = covid_data.loc[covid_data['abbreviation_canton_and_fl'] == canton, ffill_columns].ffill().fillna(0.)\n",
    "\n",
    "covid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPtCqN6xLofg"
   },
   "source": [
    "First, we specify which columns we will forward fill. Here we selected all current and cumulative counts.\n",
    "\n",
    "Then we iterate over all cantons. The `.loc` method is used for complex indexing. It can be used as `.loc[row_selection, column_selection]`. The rows we select are the ones that correspond to the current canton. The expression `covid_data['abbreviation_canton_and_fl'] == canton` returns a boolean index series that indicates rows that match the current canton. We use the list of columns we want to forward fill as our column selection.\n",
    "\n",
    "As before, we reassign all these cells by a version that has its missing values filled in. We apply the `.ffill()` method and follow it by a `fillna(0.)`. This is because missing values at the start (before any value is known) can not be forward-filled and we explicitely set them to 0 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCMWvhgANTEw"
   },
   "source": [
    "Let us now see our progess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YGVDLNUNWI4"
   },
   "outputs": [],
   "source": [
    "covid_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnFnHsMUNeCW"
   },
   "source": [
    "There are two remaining columns with missing values. For `new_hosp` we will naively fill in 0 and for `source` we will impute the string `\"unknown\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnaMZBPjx2Fu"
   },
   "source": [
    "**🚨 TASK 1A (2 Points) 🚨**\n",
    "\n",
    "* Replace missing values in the `new_hosp` column by 0\n",
    "* Replace missing values in the `source` column by the string `\"unknown\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peaTkKqzNs0o"
   },
   "outputs": [],
   "source": [
    "# Hint refer back to where we replaced missing 'time' values by \"00:00\"!\n",
    "\n",
    "covid_data['new_hosp'] = covid_data['new_hosp'].fillna(0)\n",
    "covid_data['source'] = covid_data['source'].fillna(\"unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlL0jj_LN6l9"
   },
   "source": [
    "We have now eliminated all missing values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4R1WjQvN9Lo"
   },
   "outputs": [],
   "source": [
    "covid_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GX14pnEteeXJ"
   },
   "source": [
    "<mark>In **MOODLE**</mark>:\n",
    "\n",
    "* upload your code snippte to fill in missing values for `new_hosp` and `source`\n",
    "* upload the output of counting remaining missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcHL2-ZHON8g"
   },
   "source": [
    "## Dealing with Duplicate Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wh0uWaYUOQ48"
   },
   "source": [
    "The `.duplicated()` method will tell you for each row whether it is an exact duplicate. By default it will mark the first occurrence of a row as a non-duplicate and every following occurrence as a duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-Ejmse0YRIT"
   },
   "outputs": [],
   "source": [
    "covid_data.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjDDBgVYYYou"
   },
   "source": [
    "We can check whether there are any exact duplicates by summing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hB5R0BLkYU4-"
   },
   "outputs": [],
   "source": [
    "covid_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9H65SLycYgcd"
   },
   "source": [
    "We can see that our dataset does not contain any duplicate columns, this is mainly because each entry has a unique `'date'` and `'time'`.\n",
    "\n",
    "We can set the `subset=...` parameter to indicate which columns to consider for duplicate detection. Let us check whether there are duplicates, when we ignore the `'date'` and `'time'` columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c2qk9UQytVe"
   },
   "source": [
    "**🚨 TASK 1B (1 Point) 🚨**\n",
    "\n",
    "* count the number of exact duplicates when we only consider non-time colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cFMZMszORoC"
   },
   "outputs": [],
   "source": [
    "non_time_cols = [c for c in covid_data.columns if c not in {'date', 'time'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIuRIr1t1jEX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzAGHBH2zPWo"
   },
   "outputs": [],
   "source": [
    "covid_data.duplicated(subset=non_time_cols).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5x6S2E67e6Ni"
   },
   "source": [
    "<mark>In **MOODLE**</mark>:\n",
    "* upload the number of exact duplicates considering only non-time columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aY_o5c2VZNr-"
   },
   "source": [
    "There is a good chance that the duplicates are a result of our missing value imputation approach.\n",
    "\n",
    "Here it does not seem like a good idea to drop any rows as those rows that have identical entries except date and time are not a-priori problematic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1GVriCx1euq"
   },
   "source": [
    "Nevertheless, it is good to know how to remove duplicate rows for future reference.\n",
    "\n",
    "We can do so using the `.drop_duplicates()` method, which will drop those rows that `.duplicated()` indicates as duplicates:\n",
    "\n",
    "Don't worry about running the next cell, it will return a new dataframe with rows dropped but will not change your current version of `covid_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VLn8Vx4ZMvc"
   },
   "outputs": [],
   "source": [
    "covid_data.drop_duplicates(subset=non_time_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13otm6KSa-qz"
   },
   "source": [
    "## Cleaning up the 'source' Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFgZqf-ScElX"
   },
   "source": [
    "Looking at the values in the `source` column, we can see that they correspond to URLs. Many of these belong to the same domain and only vary in their parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz6ozYz-2ASg"
   },
   "source": [
    "🚨 **TASK 1C (1 Point)** 🚨\n",
    "\n",
    "* inspect the unique values in the `source` column\n",
    "* count the unique values in the `source` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rObuK96IbERA"
   },
   "outputs": [],
   "source": [
    "# Hint: we have already done this for different column earlier\n",
    "covid_data['source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X37DH_BOf56R"
   },
   "outputs": [],
   "source": [
    "# the `.unique` method returns a numpy array, you can use the built-in `len` function to check its length\n",
    "len(covid_data['source'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8lBYndpgBUD"
   },
   "source": [
    "<mark>In **MOODLE**</mark>:\n",
    "* report the number of unique values in the `source` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5I-kcGHccbVN"
   },
   "source": [
    "Let us try making this data more digestable by extracting the domain information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKLedFgLclAw"
   },
   "source": [
    "For this we will use the following helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STSpySHsbola"
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "def extract_netloc(url):\n",
    "  return urlparse(url).netloc\n",
    "\n",
    "# lets try it out\n",
    "url = 'https://www.baselland.ch/politik-und-behorden/direktionen/volkswirtschafts-und-gesundheitsdirektion/amt-fur-gesundheit/medizinische-dienste/kantonsarztlicher-dienst/aktuelles/covid-19-faelle-kanton-basel-landschaft'\n",
    "print(extract_netloc(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StEIho00dDML"
   },
   "source": [
    "We can apply this function to the `source` column to replace it with a cleaned version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_d_ebNb_dMfv"
   },
   "outputs": [],
   "source": [
    "covid_data['source'] = covid_data['source'].apply(extract_netloc)\n",
    "covid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5wWKyoTdWTu"
   },
   "outputs": [],
   "source": [
    "sorted(covid_data['source'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvDFj1wvdixT"
   },
   "source": [
    "The `.apply` method applies the function passed as an argument to each element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0lrMNeznX79"
   },
   "source": [
    "Let's now save the final cleaned dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B09BiPKeneYY"
   },
   "outputs": [],
   "source": [
    "covid_data.to_csv('swiss_covid_data_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx8EHnlQrdzJ"
   },
   "source": [
    "At this point, we consider the data cleaning process done.\n",
    "\n",
    "ℹ️ **In the next cell, we will download a reference version of the cleaned data that you can use, if you did not manage to implement part of the process so far.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBDb86h2ryxQ"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/alisarupenyan/MLDMlabsHS2024/refs/heads/main/lab01_clean_data.csv -O lab01_clean_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-FaRxNdr_Aw"
   },
   "outputs": [],
   "source": [
    "# uncomment the next line, if you want to use reference data\n",
    "covid_data = pd.read_csv('lab01_clean_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1wRq0xhobMU"
   },
   "source": [
    "## Using the Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TK3_zZHVogrY"
   },
   "source": [
    "### **🚨 TASK 2 (3 Points) 🚨**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDGMGKaZwQkf"
   },
   "source": [
    "Now that you have a clean dataset, it is time for some data exploration.\n",
    "\n",
    "* think of a question that you could try to answer with the given dataset\n",
    "* use summary statistics and/or visualizations to try to answer your question\n",
    "\n",
    "<mark>On **Moodle**</mark> _upload a <mark>PDF</mark>_ answering the following questions:\n",
    "* What was your initial question or idea?\n",
    "* How did you proceed to arrive at an answer?\n",
    "* What are your results?\n",
    "* Include code-snippets, plots, and similar to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anhVyTsKMy3U"
   },
   "source": [
    "Below, we include some code that helps you plot time-dependent variables.\n",
    "Note that **it is _NOT_ mandatory to produce any plots for this assignment**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOie_M-NCuhp"
   },
   "outputs": [],
   "source": [
    "# You might want to plot things\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3o4I-CEJGnR"
   },
   "outputs": [],
   "source": [
    "# replace 'date' string by parsed datetime objects\n",
    "# this can help if you want to use matplotlib to create a time plot\n",
    "# you do not have to understand this\n",
    "from datetime import datetime as dt\n",
    "covid_data['date'] = pd.to_datetime(covid_data['date'].apply(lambda t: dt.strptime(t, \"%Y-%m-%d\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlYFEXr4DNe7"
   },
   "outputs": [],
   "source": [
    "# extract dates for Geneva\n",
    "x = covid_data.loc[covid_data['abbreviation_canton_and_fl'] == 'ZH', ['date']]\n",
    "# extract the 'ncumul_test' column for Geneva\n",
    "hosp = covid_data.loc[covid_data['abbreviation_canton_and_fl'] == 'ZH', ['current_hosp']]\n",
    "icu = covid_data.loc[covid_data['abbreviation_canton_and_fl'] == 'ZH', ['current_icu']]\n",
    "y = (icu['current_icu'] / hosp['current_hosp']) * 100\n",
    "\n",
    "# plot\n",
    "plt.plot_date(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0SgLr9iNYox"
   },
   "outputs": [],
   "source": [
    "# Generic scatter-plot that does not include a time variable\n",
    "plt.scatter(covid_data['ncumul_tested'], covid_data['ncumul_deceased'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
